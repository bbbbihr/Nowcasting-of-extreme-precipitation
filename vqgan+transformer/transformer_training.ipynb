{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7baff35d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\".\")\n",
    "# also disable grad to save memory\n",
    "import torch\n",
    "DEVICE = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "import yaml\n",
    "import torch\n",
    "from omegaconf import OmegaConf\n",
    "from taming.models.vqgan import VQModel, GumbelVQ\n",
    "import io\n",
    "import os, sys\n",
    "import requests\n",
    "import PIL\n",
    "from PIL import Image\n",
    "from PIL import ImageDraw, ImageFont\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as T\n",
    "import torchvision.transforms.functional as TF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "07cfb38f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define dataset\n",
    "import torch\n",
    "import sys\n",
    "from nuwa_pytorch import VQGanVAE\n",
    "import h5py\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime, timedelta\n",
    "def eventGeneration(start_time, obs_time = 3 ,lead_time = 6, time_interval = 30):\n",
    "    # Generate event based on starting time point, return a list: [[t-4,...,t-1,t], [t+1,...,t+72]]\n",
    "    # Get the start year, month, day, hour, minute\n",
    "    year = int(start_time[0:4])\n",
    "    month = int(start_time[4:6])\n",
    "    day = int(start_time[6:8])\n",
    "    hour = int(start_time[8:10])\n",
    "    minute = int(start_time[10:12])\n",
    "    #print(datetime(year=year, month=month, day=day, hour=hour, minute=minute))\n",
    "    times = [(datetime(year, month, day, hour, minute) + timedelta(minutes=time_interval * (x+1))) for x in range(lead_time)]\n",
    "    lead = [dt.strftime('%Y%m%d%H%M') for dt in times]\n",
    "    times = [(datetime(year, month, day, hour, minute) - timedelta(minutes=time_interval * x)) for x in range(obs_time)]\n",
    "    obs = [dt.strftime('%Y%m%d%H%M') for dt in times]\n",
    "    obs.reverse()\n",
    "    return lead, obs\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import h5py\n",
    "import numpy as np\n",
    "from torchvision.transforms import ToTensor, Compose, CenterCrop\n",
    "class radarDataset(Dataset):\n",
    "    def __init__(self, root_dir, event_times, obs_number = 3, pred_number = 18, transform=None):\n",
    "        # event_times is an array of starting time t(string)\n",
    "        # transform is the preprocessing functions\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        self.event_times = event_times\n",
    "        self.obs_number = obs_number\n",
    "        self.pred_number = pred_number\n",
    "    def __len__(self):\n",
    "        return len(self.event_times)\n",
    "    def __getitem__(self, idx):\n",
    "        start_time = str(self.event_times[idx])\n",
    "        time_list_pre, time_list_obs = eventGeneration(start_time, self.obs_number, self.pred_number)\n",
    "        output = []\n",
    "        time_list = time_list_obs + time_list_pre\n",
    "        #print(time_list)\n",
    "        for time in time_list:\n",
    "            year = time[0:4]\n",
    "            month = time[4:6]\n",
    "            #path = self.root_dir + year + '/' + month + '/' + 'RAD_NL25_RAC_MFBS_EM_5min_' + time + '_NL.h5'\n",
    "            path = self.root_dir + year + '/' + month + '/' + 'RAD_NL25_RAP_5min_' + time + '.h5'\n",
    "            image = np.array(h5py.File(path)['image1']['image_data'])\n",
    "            #image = np.ma.masked_where(image == 65535, image)\n",
    "            image = image[264:520,242:498]\n",
    "            image[image == 65535] = 0\n",
    "            image = image.astype('float32')\n",
    "            image = image/100*12\n",
    "            image = np.clip(image, 0, 128)\n",
    "            image = image/128\n",
    "            output.append(image)\n",
    "        output = torch.permute(torch.tensor(np.array(output)), (1, 2, 0))\n",
    "        output = self.transform(np.array(output))\n",
    "        return output\n",
    "#root_dir = '/users/hbi/data/RAD_NL25_RAC_MFBS_EM_5min/'\n",
    "#dataset = radarDataset(root_dir, [\"200808031600\"], transform = Compose([ToTensor(),CenterCrop(256)]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9a396504",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32183 3493 3560\n"
     ]
    }
   ],
   "source": [
    "# develop dataset\n",
    "from torch.cuda.amp import autocast\n",
    "from torch.autograd import Variable\n",
    "import pandas as pd\n",
    "root_dir = '/home/hbi/RAD_NL25_RAP_5min/' \n",
    "\n",
    "df_train = pd.read_csv('training_Delfland08-14_20.csv', header = None)\n",
    "event_times = df_train[0].to_list()\n",
    "dataset_train = radarDataset(root_dir, event_times, transform = Compose([ToTensor()]))  \n",
    "\n",
    "df_train_s = pd.read_csv('training_Delfland08-14.csv', header = None)\n",
    "event_times = df_train_s[0].to_list()\n",
    "dataset_train_del = radarDataset(root_dir, event_times, transform = Compose([ToTensor()]))  \n",
    "\n",
    "df_test = pd.read_csv('testing_Delfland18-20.csv', header = None)\n",
    "event_times = df_test[0].to_list()\n",
    "dataset_test = radarDataset(root_dir, event_times, transform = Compose([ToTensor()]))\n",
    "\n",
    "df_vali = pd.read_csv('validation_Delfland15-17.csv', header = None)\n",
    "event_times = df_vali[0].to_list()\n",
    "dataset_vali = radarDataset(root_dir, event_times, transform = Compose([ToTensor()]))\n",
    "\n",
    "df_train_aa = pd.read_csv('training_Aa08-14.csv', header = None)\n",
    "event_times = df_train_aa[0].to_list()\n",
    "dataset_train_aa = radarDataset(root_dir, event_times, transform = Compose([ToTensor()]))  \n",
    "\n",
    "df_train_dw = pd.read_csv('training_Dwar08-14.csv', header = None)\n",
    "event_times = df_train_dw[0].to_list()\n",
    "dataset_train_dw = radarDataset(root_dir, event_times, transform = Compose([ToTensor()]))    \n",
    "\n",
    "df_train_re = pd.read_csv('training_Regge08-14.csv', header = None)\n",
    "event_times = df_train_re[0].to_list()\n",
    "dataset_train_re = radarDataset(root_dir, event_times, transform = Compose([ToTensor()]))   \n",
    "\n",
    "print(len(dataset_train), len(dataset_test), len(dataset_vali))\n",
    "loaders = { 'train' :DataLoader(dataset_train, batch_size=1, shuffle=True, num_workers=8),\n",
    "            'test' :DataLoader(dataset_test, batch_size=1, shuffle=True, num_workers=8), \n",
    "           'valid' :DataLoader(dataset_vali, batch_size=1, shuffle=False, num_workers=8),\n",
    "          \n",
    "          'train_aa5' :DataLoader(dataset_train_aa, batch_size=1, shuffle=False, num_workers=8),\n",
    "          'train_dw5' :DataLoader(dataset_train_dw, batch_size=1, shuffle=False, num_workers=8),\n",
    "          'train_del5' :DataLoader(dataset_train_del, batch_size=1, shuffle=False, num_workers=8),\n",
    "          'train_re5' :DataLoader(dataset_train_re, batch_size=1, shuffle=False, num_workers=8),\n",
    "          }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4e05abba",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "from nuwa_pytorch.nuwa_pytorch import Sparse3DNA, Attention, SparseCross2DNA, cast_tuple, mult_reduce, calc_same_padding, unfoldNd, FeedForward, ShiftVideoTokens, SandwichNorm, StableLayerNorm\n",
    "class SparseCross3DNA(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        dim,\n",
    "        video_shape,\n",
    "        kernel_size = 3,\n",
    "        dilation = 1,\n",
    "        heads = 8,\n",
    "        dim_head = 64,\n",
    "        dropout = 0.,\n",
    "        causal = False,\n",
    "        query_num_frames_chunk = None,\n",
    "        rel_pos_bias = False\n",
    "    ):\n",
    "        super().__init__()\n",
    "        inner_dim = dim_head * heads\n",
    "        self.heads = heads\n",
    "        self.scale = dim_head ** -0.5\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.to_q = nn.Linear(dim, inner_dim, bias = False)\n",
    "        self.to_kv = nn.Linear(dim, inner_dim * 2, bias = False)\n",
    "        self.talking_heads = nn.Conv2d(heads, heads, 1, bias = False)\n",
    "        self.to_out = nn.Linear(inner_dim, dim)\n",
    "        self.dilation = cast_tuple(dilation, size = 3)\n",
    "        self.kernel_size = cast_tuple(kernel_size, size = 3)\n",
    "        self.kernel_numel = mult_reduce(self.kernel_size)\n",
    "       # relative positional bias per head, if needed\n",
    "        self.rel_pos_bias = AxialPositionalEmbedding(heads, shape = self.kernel_size) if rel_pos_bias else None\n",
    "        # calculate padding\n",
    "        self.padding_frame = calc_same_padding(self.kernel_size[0], self.dilation[0])\n",
    "        self.padding_height = calc_same_padding(self.kernel_size[1], self.dilation[1])\n",
    "        self.padding_width = calc_same_padding(self.kernel_size[2], self.dilation[2])\n",
    "        self.video_padding = (self.padding_width, self.padding_width, self.padding_height, self.padding_height, self.padding_frame, self.padding_frame)\n",
    "        # save video shape and calculate max number of tokens\n",
    "        self.video_shape = video_shape\n",
    "        max_frames, fmap_size, _ = video_shape\n",
    "        max_num_tokens = torch.empty(video_shape).numel()\n",
    "        self.max_num_tokens = max_num_tokens\n",
    "        # how many query tokens to process at once to limit peak memory usage, by multiple of frame tokens (fmap_size ** 2)\n",
    "        self.query_num_frames_chunk = default(query_num_frames_chunk, max_frames)\n",
    "        # precalculate causal mask\n",
    "        indices = torch.arange(max_num_tokens)\n",
    "        shaped_indices = rearrange(indices, '(f h w) -> 1 1 f h w', f = max_frames, h = fmap_size, w = fmap_size)\n",
    "        padded_indices = F.pad(shaped_indices, self.video_padding, value = max_num_tokens) # padding has value of max tokens so to be masked out\n",
    "        unfolded_indices = unfoldNd(padded_indices, kernel_size = self.kernel_size, dilation = self.dilation)\n",
    "        unfolded_indices = rearrange(unfolded_indices, '1 k n -> n k')\n",
    "        # if causal, compare query and key indices and make sure past cannot see future\n",
    "        # if not causal, just mask out the padding\n",
    "\n",
    "        if causal:\n",
    "            mask = rearrange(indices, 'n -> n 1') < unfolded_indices\n",
    "        else:\n",
    "            mask = unfolded_indices == max_num_tokens\n",
    "\n",
    "        #mask = F.pad(mask, (1, 0), value = False) # bos tokens never get masked out\n",
    "        self.register_buffer('mask', mask)\n",
    "\n",
    "    def forward(self, x, context, **kwargs):\n",
    "        b, n, _, h, device = *x.shape, self.heads, x.device\n",
    "        n_context = context.shape[1]\n",
    "        # more variables\n",
    "\n",
    "        dilation = self.dilation\n",
    "        kernel_size = self.kernel_size\n",
    "        video_padding = self.video_padding\n",
    "        fmap_size = self.video_shape[1]\n",
    "        tokens_per_frame = fmap_size ** 2\n",
    "\n",
    "        padding = padding_to_multiple_of(n - 1, tokens_per_frame)\n",
    "        num_frames = (n + padding) // tokens_per_frame\n",
    "        num_frames_input = (n_context + padding) // tokens_per_frame\n",
    "        \n",
    "        # derive queries / keys / values\n",
    "\n",
    "        q, k, v = (self.to_q(x), *self.to_kv(context).chunk(2, dim = -1))\n",
    "\n",
    "        # split out heads\n",
    "\n",
    "        q, k, v = map(lambda t: rearrange(t, 'b n (h d) -> (b h) n d', h = h), (q, k, v))\n",
    "        \n",
    "        # take care of bos\n",
    "\n",
    "        #q = q[:, 1:]\n",
    "        #bos_value = q[:, :1]\n",
    "        \n",
    "        # scale queries\n",
    "        q = q * self.scale\n",
    "\n",
    "        # reshape keys and values to video and add appropriate padding along all dimensions (frames, height, width)\n",
    "        k, v = map(lambda t: rearrange(t, 'b (f h w) d -> b d f h w',  h = fmap_size, w = fmap_size), (k, v))\n",
    "        k, v = map(lambda t: F.pad(t, video_padding), (k, v))\n",
    "        #print(k.shape)\n",
    "        # axial relative pos bias\n",
    "\n",
    "        rel_pos_bias = None\n",
    "\n",
    "        if exists(self.rel_pos_bias):\n",
    "            rel_pos_bias = rearrange(self.rel_pos_bias(), 'j h -> h 1 j')\n",
    "            rel_pos_bias = F.pad(rel_pos_bias, (1, 0), value = 0.)\n",
    "\n",
    "        # put the attention processing code in a function\n",
    "        # to allow for processing queries in chunks of frames\n",
    "\n",
    "        out = []\n",
    "\n",
    "        def attend(q, k, v, mask, kernel_size):\n",
    "            chunk_length = q.shape[1]\n",
    "\n",
    "            k, v = map(lambda t: unfoldNd(t, kernel_size = kernel_size, dilation = dilation), (k, v))\n",
    "            k, v = map(lambda t: rearrange(t, 'b (d j) i -> b i j d', j = self.kernel_numel), (k, v))\n",
    "            k, v = map(lambda t: t[:, :chunk_length], (k, v))\n",
    "\n",
    "            # calculate sim\n",
    "\n",
    "            sim = einsum('b i d, b i j d -> b i j', q, k)\n",
    "\n",
    "            # add rel pos bias, if needed\n",
    "\n",
    "            if exists(rel_pos_bias):\n",
    "                sim = sim + rel_pos_bias\n",
    "\n",
    "            # causal mask\n",
    "\n",
    "            if exists(mask):\n",
    "                mask_value = -torch.finfo(sim.dtype).max\n",
    "                mask = rearrange(mask, 'i j -> 1 i j')\n",
    "                sim = sim.masked_fill(mask, mask_value)\n",
    "\n",
    "            # attention\n",
    "\n",
    "            attn = stable_softmax(sim, dim = -1)\n",
    "\n",
    "            attn = rearrange(attn, '(b h) ... -> b h ...', h = h)\n",
    "            attn = self.talking_heads(attn)\n",
    "            attn = rearrange(attn, 'b h ... -> (b h) ...')\n",
    "\n",
    "            attn = self.dropout(attn)\n",
    "\n",
    "            # aggregate values\n",
    "\n",
    "            return einsum('b i j, b i j d -> b i d', attn, v)\n",
    "\n",
    "        # process queries in chunks\n",
    "\n",
    "        frames_per_chunk = min(self.query_num_frames_chunk, num_frames)\n",
    "        chunk_size = frames_per_chunk * tokens_per_frame\n",
    "        q_chunks = q.split(chunk_size, dim = 1)\n",
    "        \n",
    "        mask = self.mask\n",
    "\n",
    "        for ind, q_chunk in enumerate(q_chunks):\n",
    "            #print(ind, q_chunks[ind].shape, mask.shape)\n",
    "            q_chunk = q_chunks[ind]\n",
    "            mask_chunk = mask\n",
    "            #print(q_chunk.shape, mask.shape, k.shape, v.shape)\n",
    "            # slice the keys and values to the appropriate frames, accounting for padding along frames dimension\n",
    "\n",
    "            kv_start_pos = ind * frames_per_chunk\n",
    "            kv_end_pos = kv_start_pos + (ind + frames_per_chunk + self.padding_frame * 2)\n",
    "            kv_frame_range = slice(kv_start_pos, kv_end_pos)\n",
    "\n",
    "            k_slice, v_slice = map(lambda t: t[:, :, kv_frame_range], (k, v))\n",
    "            k_slice, v_slice = k,v\n",
    "            # calculate output chunk\n",
    "            out_chunk = attend(\n",
    "                q = q_chunk,\n",
    "                k = k_slice,\n",
    "                v = v_slice,\n",
    "                mask = mask_chunk,\n",
    "                kernel_size = kernel_size,\n",
    "            )\n",
    "            out.append(out_chunk)\n",
    "\n",
    "        # combine all chunks\n",
    "        out = torch.cat(out, dim = 1)\n",
    "        # append bos value\n",
    "\n",
    "        #out = torch.cat((bos_value, out), dim = 1)  # bos will always adopt its own value, since it pays attention only to itself\n",
    "        # merge heads\n",
    "\n",
    "        out = rearrange(out, '(b h) n d -> b n (h d)', h = h)\n",
    "        return self.to_out(out)\n",
    "    \n",
    "class Transformer(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        *,\n",
    "        dim,\n",
    "        depth,\n",
    "        causal = False,\n",
    "        heads = 8,\n",
    "        dim_head = 64,\n",
    "        ff_mult = 4,\n",
    "        cross_attend = False,\n",
    "        attn_dropout = 0.,\n",
    "        ff_dropout = 0.,\n",
    "        ff_chunk_size = None,\n",
    "        cross_2dna_attn = False,\n",
    "        cross_2dna_image_size = None,\n",
    "        cross_2dna_kernel_size = 3,\n",
    "        cross_2dna_dilations = (1,),\n",
    "        cross_3dna_attn = False,\n",
    "        cross_3dna_image_size = None,\n",
    "        cross_3dna_kernel_size = 5,\n",
    "        cross_3dna_dilations = (1,),\n",
    "        sparse_3dna_attn = False,\n",
    "        sparse_3dna_kernel_size = 3,\n",
    "        sparse_3dna_video_shape = None,\n",
    "        sparse_3dna_query_num_frames_chunk = None,\n",
    "        sparse_3dna_dilations = (1,),\n",
    "        sparse_3dna_rel_pos_bias = False,\n",
    "        shift_video_tokens = False,\n",
    "        rotary_pos_emb = False\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.layers = MList([])\n",
    "\n",
    "        for ind in range(depth):\n",
    "            if sparse_3dna_attn:\n",
    "                dilation = sparse_3dna_dilations[ind % len(sparse_3dna_dilations)]\n",
    "\n",
    "                self_attn = Sparse3DNA(\n",
    "                    dim = dim,\n",
    "                    heads = heads,\n",
    "                    dim_head = dim_head,\n",
    "                    causal = causal,\n",
    "                    kernel_size = sparse_3dna_kernel_size,\n",
    "                    dilation = dilation,\n",
    "                    video_shape = sparse_3dna_video_shape,\n",
    "                    query_num_frames_chunk = sparse_3dna_query_num_frames_chunk,\n",
    "                    rel_pos_bias = sparse_3dna_rel_pos_bias,\n",
    "                )\n",
    "            else:\n",
    "                self_attn = Attention(\n",
    "                    dim = dim,\n",
    "                    heads = heads,\n",
    "                    dim_head = dim_head,\n",
    "                    causal = causal,\n",
    "                    dropout = attn_dropout\n",
    "                )\n",
    "\n",
    "            cross_attn = None\n",
    "\n",
    "            if cross_attend:\n",
    "                if cross_2dna_attn:\n",
    "                    dilation = cross_2dna_dilations[ind % len(cross_2dna_dilations)]\n",
    "\n",
    "                    cross_attn = SparseCross2DNA(\n",
    "                        dim = dim,\n",
    "                        heads = heads,\n",
    "                        dim_head = dim_head,\n",
    "                        dropout = attn_dropout,\n",
    "                        image_size = cross_2dna_image_size,\n",
    "                        kernel_size = cross_2dna_kernel_size,\n",
    "                        dilation = dilation\n",
    "                    )\n",
    "                \n",
    "                elif cross_3dna_attn:\n",
    "                    \n",
    "                    cross_attn = SparseCross3DNA(\n",
    "                        dim = dim,\n",
    "                        heads = heads,\n",
    "                        dim_head = dim_head,\n",
    "                        dropout = attn_dropout,\n",
    "                        video_shape = cross_3dna_image_size,\n",
    "                        kernel_size = cross_3dna_kernel_size,\n",
    "                    )\n",
    "                \n",
    "                else:\n",
    "                    cross_attn = Attention(\n",
    "                        dim = dim,\n",
    "                        heads = heads,\n",
    "                        dim_head = dim_head,\n",
    "                        dropout = attn_dropout\n",
    "                    )\n",
    "\n",
    "            ff = FeedForward(dim = dim, mult = ff_mult, dropout = ff_dropout, chunk_size = ff_chunk_size)\n",
    "\n",
    "            if sparse_3dna_attn and shift_video_tokens:\n",
    "                fmap_size = sparse_3dna_video_shape[-1]\n",
    "                self_attn = ShiftVideoTokens(self_attn, image_size = fmap_size)\n",
    "                ff        = ShiftVideoTokens(ff, image_size = fmap_size)\n",
    "\n",
    "            self.layers.append(MList([\n",
    "                SandwichNorm(dim = dim, fn = self_attn),\n",
    "                SandwichNorm(dim = dim, fn = cross_attn) if cross_attend else None,\n",
    "                SandwichNorm(dim = dim, fn = ff)\n",
    "            ]))\n",
    "\n",
    "        self.norm = StableLayerNorm(dim)\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        x,\n",
    "        mask = None,\n",
    "        context = None,\n",
    "        context_mask = None\n",
    "    ):\n",
    "        for attn, cross_attn, ff in self.layers:\n",
    "            x = attn(x, mask = mask) + x\n",
    "\n",
    "            if exists(cross_attn):\n",
    "                x = cross_attn(x, context = context, mask = mask, context_mask = context_mask) + x\n",
    "\n",
    "            x = ff(x) + x\n",
    "\n",
    "        return self.norm(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7ab99fef",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1024])\n"
     ]
    }
   ],
   "source": [
    "weight = {676: 1286569, 674: 706515, 129: 367104, 178: 289731, 267: 252592, 33: 244407, 105: 186288, 657: 148341, 232: 127132, 148: 125003, 409: 106906, 577: 105078, 197: 103030, 408: 98176, 115: 94823, 302: 90022, 896: 85156, 752: 81477, 23: 74606, 450: 74520, 553: 72955, 82: 72498, 584: 71854, 484: 69222, 855: 60575, 45: 58398, 135: 56177, 708: 53631, 406: 52498, 124: 52310, 503: 51673, 652: 50737, 564: 50727, 654: 50581, 798: 50264, 587: 49564, 184: 48388, 505: 47029, 880: 46506, 474: 46405, 970: 46203, 428: 46016, 159: 46003, 380: 44474, 715: 44128, 369: 43548, 728: 42507, 303: 42143, 25: 41416, 994: 41078, 261: 41035, 336: 40827, 663: 40308, 968: 40199, 181: 39830, 283: 39522, 128: 39213, 354: 38859, 819: 37764, 44: 37715, 417: 37550, 540: 37264, 595: 37020, 392: 37009, 582: 36752, 222: 36533, 501: 36164, 217: 35185, 101: 34958, 800: 34712, 653: 34639, 114: 34391, 956: 33259, 607: 32589, 216: 32503, 27: 32237, 1007: 32015, 744: 31891, 109: 31707, 499: 31690, 1013: 31499, 658: 31273, 358: 31153, 784: 30926, 103: 29705, 659: 29129, 562: 28409, 988: 27714, 198: 27196, 457: 27012, 231: 26399, 816: 25858, 603: 23635, 452: 23135, 160: 22420, 171: 21585, 891: 20388, 723: 19264, 844: 17305, 610: 17243, 797: 16665, 460: 16509, 290: 14949, 704: 95, 13: 2}\n",
    "total = 0\n",
    "for key in weight:\n",
    "    total += weight[key]\n",
    "for key in weight:\n",
    "    weight[key] =(weight[key]/total*100)\n",
    "w = [weight[min(weight)]]*1024\n",
    "for key in weight:\n",
    "    w[key] = weight[key]\n",
    "for i in range(1024):\n",
    "    w[i] = min(1/w[i], 1000)\n",
    "w = torch.tensor(w).unsqueeze(0).to(DEVICE)\n",
    "print(w.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "28d7f3ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# evt modules\n",
    "from torch import nn\n",
    "from nuwa_pytorch.nuwa_pytorch import Embedding\n",
    "from einops import rearrange\n",
    "# gt_indicator: \n",
    "# indicator: [n, 1024] logits + [1, 1024] masks \n",
    "# gamma: hyperparameter\n",
    "# beta0: propotion of non-extreme tokens\n",
    "# beta1: propotion of extreme tokens \n",
    "\n",
    "def cal_evt_loss(indicator, gt_indicator, gamma = 1, beta0 = 0.95, beta1 = 0.05):\n",
    "    loss1 = -1 * beta0 * torch.pow((1-indicator/gamma),gamma) * gt_indicator * torch.log(indicator)\n",
    "    loss2 = -1 * beta1 * torch.pow((1-(1-indicator)/gamma),gamma) * (1-gt_indicator) * torch.log(1-indicator)\n",
    "    loss = loss1 + loss2 \n",
    "    return loss\n",
    "\n",
    "# logits b * 1024 * 1536\n",
    "# pre_token b * 1 * 1536\n",
    "# ext_tokens 1*15\n",
    "def evt_loss(logits, gt_tokens, ext_tokens, gamma = 2, beta0 = 0.95, beta1 = 0.05):\n",
    "    batch = logits.shape[0]\n",
    "    channel = logits.shape[1]\n",
    "    number = logits.shape[2]\n",
    "    device = logits.device\n",
    "    softmax = nn.Softmax(dim = -1)\n",
    "    loss = 0\n",
    "    count = 0\n",
    "    for batch_id in range(batch):\n",
    "        for n in range(number):\n",
    "            gt = gt_tokens[batch_id,n]\n",
    "            if gt not in ext_tokens: continue\n",
    "            gt_indicator = 1\n",
    "            prob = softmax(logits[batch_id,:,n])\n",
    "            indicator = 0\n",
    "            for token in ext_tokens:\n",
    "                indicator += prob[token]\n",
    "            loss += cal_evt_loss(indicator, gt_indicator, gamma, beta0, beta1)\n",
    "            count += 1\n",
    "    return loss/count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "006c7ec1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# transformer for second stage\n",
    "# transformer for second stage\n",
    "from torch import nn\n",
    "import nuwa_pytorch.nuwa_pytorch\n",
    "from nuwa_pytorch.nuwa_pytorch import MList, Embedding, AxialPositionalEmbedding, ReversibleTransformer, default, eval_decorator, prob_mask_like, exists\n",
    "from nuwa_pytorch.nuwa_pytorch import padding_to_multiple_of, einsum, stable_softmax\n",
    "from einops import rearrange, repeat\n",
    "import torch.nn.functional as F\n",
    "from main import instantiate_from_config\n",
    "from dice_loss import DiceLoss\n",
    "class nuwa_v2v(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        *,\n",
    "        dim,\n",
    "        vae = None,\n",
    "        image_size = 256,\n",
    "        codebook_size = 1024,\n",
    "        compression_rate = 16,\n",
    "        video_out_seq_len = 5,\n",
    "        video_in_seq_len = 5,\n",
    "        video_dec_depth = 6,\n",
    "        video_dec_dim_head = 64,\n",
    "        video_dec_heads = 8,\n",
    "        attn_dropout = 0.,\n",
    "        ff_dropout = 0.,\n",
    "        ff_chunk_size = None,\n",
    "        embed_gradient_frac = 0.2,\n",
    "        shift_video_tokens = True,\n",
    "        sparse_3dna_kernel_size = 3,\n",
    "        sparse_3dna_query_num_frames_chunk = None,\n",
    "        sparse_3dna_dilation = 1,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        \n",
    "        # VAE\n",
    "        #self.first_stage_model = vae.copy_for_eval()\n",
    "        self.image_size = image_size\n",
    "        num_image_tokens = codebook_size\n",
    "        fmap_size = image_size // compression_rate\n",
    "        self.video_fmap_size = fmap_size\n",
    "        self.video_frames_in = video_in_seq_len\n",
    "        self.video_frames_out = video_out_seq_len\n",
    "        self.image_embedding = Embedding(num_image_tokens, dim, frac_gradient=embed_gradient_frac)\n",
    "        # cycle dilation for sparse 3d-nearby attention \n",
    "        sparse_3dna_dilations = tuple(range(1, sparse_3dna_dilation + 1)) if not isinstance(sparse_3dna_dilation, (list, tuple)) else sparse_3dna_dilation  # ???\n",
    "        \n",
    "        video_shape_in = (video_in_seq_len, fmap_size, fmap_size)\n",
    "        video_shape_out = (video_out_seq_len, fmap_size, fmap_size)\n",
    "        self.video_in_pos_emb = AxialPositionalEmbedding(dim, shape = video_shape_in)\n",
    "        self.video_out_pos_emb = AxialPositionalEmbedding(dim, shape = video_shape_out)\n",
    "        self.video_bos = nn.Parameter(torch.randn(dim))\n",
    "        \n",
    "        # 3DNA Encoder\n",
    "        video_shape_input = (video_in_seq_len, fmap_size, fmap_size)\n",
    "    \n",
    "        # 3DNA Decoder\n",
    "        #self.video_bos = nn.Parameter(torch.randn(dim))#\n",
    "        #self.image_embedding = Embedding(num_image_tokens, dim, frac_gradient = embed_gradient_frac)#???\n",
    "        # self.video_pos_emb = AxialPositionalEmbedding(dim, shape = video_shape)#???\n",
    "\n",
    "        video_shape_output = (video_out_seq_len, fmap_size, fmap_size)\n",
    "        self.video_decode_transformer = Transformer(\n",
    "            dim = dim,\n",
    "            depth = video_dec_depth,\n",
    "            heads = video_dec_heads,\n",
    "            dim_head = video_dec_dim_head,\n",
    "            causal = True,\n",
    "            cross_attend = True,\n",
    "            attn_dropout = attn_dropout,\n",
    "            ff_dropout = ff_dropout,\n",
    "            ff_chunk_size = ff_chunk_size,\n",
    "            cross_3dna_attn = True,\n",
    "            cross_3dna_image_size = video_shape_input ,\n",
    "            cross_3dna_kernel_size = (5,3,3),\n",
    "            shift_video_tokens = shift_video_tokens,\n",
    "            sparse_3dna_video_shape = video_shape_output,\n",
    "            sparse_3dna_attn = True,\n",
    "            sparse_3dna_kernel_size = sparse_3dna_kernel_size,\n",
    "            sparse_3dna_dilations = sparse_3dna_dilations,\n",
    "            sparse_3dna_query_num_frames_chunk = sparse_3dna_query_num_frames_chunk\n",
    "        )\n",
    "\n",
    "        self.to_logits = nn.Linear(dim, num_image_tokens)\n",
    "    \n",
    "    @torch.no_grad()\n",
    "    @eval_decorator\n",
    "    def generate(\n",
    "        self,\n",
    "        *,\n",
    "        video_input_latent,\n",
    "        filter_thres = 0.9,\n",
    "        temperature = 1.,\n",
    "        num_frames = 6\n",
    "    ):\n",
    "        device = video_input_latent.device\n",
    "        batch = video_input_latent.shape[0]\n",
    "        \n",
    "        frame_indices_input = video_input_latent.squeeze(1)\n",
    "        frame_embeddings_input = self.image_embedding(frame_indices_input)\n",
    "        frame_embeddings_input = self.video_in_pos_emb().repeat(batch,1,1) + frame_embeddings_input\n",
    "        \n",
    "        bos = repeat(self.video_bos, 'd -> b 1 d', b = batch)\n",
    "        video_indices = torch.empty((batch, 0), device = device, dtype = torch.long)\n",
    "        num_tokens_per_frame = self.video_fmap_size ** 2\n",
    "        \n",
    "        num_frames = default(num_frames, self.video_frames_out)\n",
    "        total_video_tokens =  num_tokens_per_frame * num_frames\n",
    "\n",
    "        pos_emb = self.video_out_pos_emb()\n",
    "\n",
    "        for ind in range(total_video_tokens):\n",
    "            print(ind, '/', total_video_tokens, end = '\\r')\n",
    "            video_indices_input = video_indices\n",
    "            num_video_tokens = video_indices.shape[1]\n",
    "            frame_embeddings = self.image_embedding(video_indices_input)\n",
    "            frame_embeddings = pos_emb[:frame_embeddings.shape[1]] + frame_embeddings\n",
    "            frame_embeddings = torch.cat((bos, frame_embeddings), dim = 1)\n",
    "\n",
    "            frame_embeddings = self.video_decode_transformer(\n",
    "                frame_embeddings,\n",
    "                context = frame_embeddings_input\n",
    "            )\n",
    "\n",
    "            logits = self.to_logits(frame_embeddings)\n",
    "            logits = logits[:, -1, :]\n",
    "            filtered_logits = top_k(logits, thres = filter_thres)\n",
    "            sample = gumbel_sample(filtered_logits, temperature = temperature, dim = -1)\n",
    "            sample = rearrange(sample, 'b -> b 1')\n",
    "            video_indices = torch.cat((video_indices, sample), dim = 1)\n",
    "\n",
    "        return video_indices\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        *,\n",
    "        #video_input, # raw observation video here\n",
    "        #video_output, # raw prediction video here\n",
    "        video_input_latent,\n",
    "        video_output_latent,\n",
    "        return_loss = False,\n",
    "        cond_dropout_prob = 0,\n",
    "        use_evt_loss = False,\n",
    "        evt_loss_weight = 0.5\n",
    "    ):\n",
    "        device = video_input_latent.device\n",
    "        batch = video_input_latent.shape[0]\n",
    "        seq_out_len = self.video_frames_out\n",
    "        seq_in_len = self.video_frames_in\n",
    "        \n",
    "        #get indices\n",
    "        frame_indices_input = video_input_latent.squeeze(1)\n",
    "        frame_indices_output = video_output_latent.squeeze(1)\n",
    "        frame_indices_output = frame_indices_output[:, :-1] if return_loss else frame_indices_output\n",
    "        \n",
    "        #indices to embedding\n",
    "        frame_embeddings_input = self.image_embedding(frame_indices_input)\n",
    "        frame_embeddings_prediction = self.image_embedding(frame_indices_output)\n",
    "\n",
    "        #position encoding\n",
    "        frame_embeddings_input = self.video_in_pos_emb().repeat(batch,1,1) + frame_embeddings_input\n",
    "        if return_loss:\n",
    "            frame_embeddings_prediction = self.video_out_pos_emb()[:-1].repeat(batch,1,1) + frame_embeddings_prediction\n",
    "            # shift right\n",
    "            bos = repeat(self.video_bos, 'd -> b 1 d', b = batch)\n",
    "            frame_embeddings_prediction = torch.cat((bos, frame_embeddings_prediction), dim = 1)\n",
    "        else:\n",
    "            frame_embeddings_prediction = self.video_out_pos_emb().repeat(batch,1,1) + frame_embeddings_prediction\n",
    "        \n",
    "        #transformer \n",
    "        frame_embeddings_prediction = self.video_decode_transformer(\n",
    "            frame_embeddings_prediction,\n",
    "            context = frame_embeddings_input\n",
    "        )\n",
    "        logits = self.to_logits(frame_embeddings_prediction)\n",
    "        \n",
    "        if not return_loss:\n",
    "            return logits\n",
    "        loss = F.cross_entropy(rearrange(logits, 'b n c -> b c n'), video_output_latent.squeeze(1))\n",
    "        total_loss = loss\n",
    "        evt_loss_term = 0 \n",
    "        if use_evt_loss:\n",
    "            ext_tokens = torch.tensor([452, 303, 171, 653, 891, 603, 499, 160, 216, 109, 290, 797, 956, 607, 816]).to(DEVICE)\n",
    "            evt_loss_term = evt_loss_weight * evt_loss(rearrange(logits, 'b n c -> b c n'),  \n",
    "                                               video_output_latent.squeeze(1), \n",
    "                                               ext_tokens)  \n",
    "            if not torch.isnan(evt_loss_term):\n",
    "                total_loss = loss + evt_loss_weight *  evt_loss_term    \n",
    "        \n",
    "        return total_loss, loss, logits, evt_loss_term"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f2216183",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7575\n"
     ]
    }
   ],
   "source": [
    "class latentDataset(Dataset):\n",
    "    def __init__(self, root_dir, radarset):\n",
    "        # event_times is an array of starting time t(string)\n",
    "        # transform is the preprocessing functions\n",
    "        self.root_dir = root_dir\n",
    "        self.radarset = radarset\n",
    "    def __len__(self):\n",
    "        if self.radarset: return len(self.radarset)\n",
    "        else: return 30000\n",
    "    def __getitem__(self, idx):\n",
    "        dir_file = self.root_dir + str(idx) + '.pt'\n",
    "        if os.path.exists(dir_file):\n",
    "            video_in_latent, video_out_latent = torch.load(dir_file, map_location='cpu')\n",
    "            return video_in_latent, video_out_latent\n",
    "        else:\n",
    "            print(\"File not found\")\n",
    "            return None\n",
    "\n",
    "latent_valid = latentDataset('/home/hbi/vali1517_newvae/validset', dataset_vali)\n",
    "latent_test = latentDataset('/home/hbi/test1820/testset', dataset_test)\n",
    "latent_train_de5  = latentDataset('/home/hbi/train0814_Delf_newvae/trainset', dataset_train_del)\n",
    "latent_train_aa5  = latentDataset('/home/hbi/train0814_Aa_newvae/trainset', dataset_train_aa)\n",
    "latent_train_dw5  = latentDataset('/home/hbi/train0814_Dwar_newvae/trainset', dataset_train_dw)\n",
    "latent_train_re5 = latentDataset('/home/hbi/train0814_Regge_newvae/trainset', dataset_train_re)\n",
    "latent_list = [latent_train_de5, latent_train_aa5, latent_train_dw5, latent_train_re5]\n",
    "latent_train_aadedwre = torch.utils.data.ConcatDataset(latent_list)\n",
    "\n",
    "loaders_latent = { 'test' :DataLoader(latent_test, batch_size=1, shuffle=True, num_workers=0),\n",
    "                  'train_aadedwre' :DataLoader(latent_train_aadedwre, batch_size=1, shuffle=True, num_workers=0),\n",
    "                  #'train_del5' :DataLoader(latent_train_de5, batch_size=1, shuffle=True, num_workers=0),\n",
    "                  'train_aa5' :DataLoader(latent_train_aa5, batch_size=1, shuffle=True, num_workers=0)}\n",
    "print(len(loaders_latent['train_aa5']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c1d5c925",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=================================================================\n",
      "Layer (type:depth-idx)                   Param #\n",
      "=================================================================\n",
      "├─Embedding: 1-1                         --\n",
      "|    └─Embedding: 2-1                    524,288\n",
      "├─AxialPositionalEmbedding: 1-2          17,920\n",
      "├─AxialPositionalEmbedding: 1-3          19,456\n",
      "├─Transformer: 1-4                       --\n",
      "|    └─ModuleList: 2-2                   --\n",
      "|    |    └─ModuleList: 3-1              3,155,658\n",
      "|    |    └─ModuleList: 3-2              3,155,658\n",
      "|    |    └─ModuleList: 3-3              3,155,658\n",
      "|    |    └─ModuleList: 3-4              3,155,658\n",
      "|    |    └─ModuleList: 3-5              3,155,658\n",
      "|    |    └─ModuleList: 3-6              3,155,658\n",
      "|    |    └─ModuleList: 3-7              3,155,658\n",
      "|    |    └─ModuleList: 3-8              3,155,658\n",
      "|    |    └─ModuleList: 3-9              3,155,658\n",
      "|    |    └─ModuleList: 3-10             3,155,658\n",
      "|    |    └─ModuleList: 3-11             3,155,658\n",
      "|    |    └─ModuleList: 3-12             3,155,658\n",
      "|    └─StableLayerNorm: 2-3              --\n",
      "|    |    └─LayerNorm: 3-13              1,024\n",
      "├─Linear: 1-5                            525,312\n",
      "=================================================================\n",
      "Total params: 38,955,896\n",
      "Trainable params: 38,955,896\n",
      "Non-trainable params: 0\n",
      "=================================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from torchsummary import summary\n",
    "from torch import optim\n",
    "#torch.cuda.empty_cache()\n",
    "nuwa = nuwa_v2v(dim=512 , vae=None,\n",
    "                image_size = 256,\n",
    "                codebook_size = 1024,\n",
    "                compression_rate = 16,\n",
    "                video_out_seq_len = 6,\n",
    "                video_in_seq_len = 3,\n",
    "                video_dec_depth = 12,\n",
    "                video_dec_dim_head = 64,\n",
    "                video_dec_heads = 4,\n",
    "                sparse_3dna_kernel_size = (7,3,3),\n",
    "                attn_dropout = 0.2,\n",
    "                ff_dropout = 0.2).to(DEVICE)\n",
    "#optimizer = optim.Adam(nuwa.parameters(),lr = 1e-3, weight_decay = 1e-6)\n",
    "optimizer = optim.AdamW(nuwa.parameters(), lr = 1e-3, weight_decay = 0.1)\n",
    "summary(nuwa)\n",
    "print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "07d74306",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1012 /37685, loss: 1.5290, evl: 0.4733\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from tqdm import tqdm\n",
    "#from pysteps.visualization import plot_precip_field\n",
    "nuwa.to(DEVICE)\n",
    "#torch.cuda.empty_cache()\n",
    "num_epochs = 10\n",
    "total_step = len(loaders_latent['train_aadedwre'])\n",
    "loss_sum = 0\n",
    "evl_sum = 0\n",
    "count = 0\n",
    "ga = 64\n",
    "number = 12 \n",
    "\n",
    "for g in optimizer.param_groups:\n",
    "    g['lr'] = 5e-4\n",
    "    g['weight_decay'] = 0.1\n",
    "for epoch in range(num_epochs):\n",
    "    for i, latent in enumerate(loaders_latent['train_aadedwre']):\n",
    "        nuwa.train()\n",
    "        torch.set_grad_enabled(True)\n",
    "        #if i<0:continue\n",
    "        #if i>=1:break\n",
    "        latent_in = latent[0].to(DEVICE)\n",
    "        latent_out = latent[1].to(DEVICE)\n",
    "        \n",
    "        total_loss, loss, _, evt_loss_term = nuwa(video_input_latent = latent_in,\n",
    "            video_output_latent = latent_out, \n",
    "            return_loss = True,\n",
    "            cond_dropout_prob = 0.1,\n",
    "            use_evt_loss = True,\n",
    "            evt_loss_weight = 0.75)\n",
    "        (total_loss / ga).backward()\n",
    "        \n",
    "        if (i+1)%ga == 0:\n",
    "            torch.nn.utils.clip_grad_norm_(nuwa.parameters(), 0.5)\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "        \n",
    "        loss_sum += float(total_loss.item())\n",
    "        evl_sum += float(evt_loss_term)\n",
    "        count += 1\n",
    "        \n",
    "        # backpropagation, compute gradients   \n",
    "        print( (i+1) % total_step, '/{}, loss: {:.4f}, evl: {:.4f}'.format(total_step, loss_sum/count, evt_loss_term), end='\\r')\n",
    "        \n",
    "        if (i+1) % total_step == 0:\n",
    "            print ('Epoch [{}/{}], Step [{}/{}], Average Loss: {:.4f}' \n",
    "                   .format(epoch + 1, num_epochs, i + 1, total_step, loss_sum/count))\n",
    "            loss_sum = 0\n",
    "            count = 0 \n",
    "            # save check point\n",
    "            torch.save(nuwa.state_dict(),'nuwa_newvae_sparse7_evl_75_epoch{}'.format(number))\n",
    "            torch.save(optimizer.state_dict(), 'nuwa_newvae_sparse7_evl_75_epoch{}_optim'.format(number))\n",
    "            for g in optimizer.param_groups:\n",
    "                g['lr'] = g['lr']*0.98\n",
    "            number += 1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
